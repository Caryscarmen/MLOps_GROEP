Job ID: 18506892
Node: gcn9
CUDA: True
Starting Experiment: mlp_baseline
Using device: cuda
Loading Data...
Initializing Model...
Starting Training...
DEBUG: Asking DataLoader for a batch...
Epoch 1 | Batch 0/4916 | Loss: 0.6909
Epoch 1 | Batch 100/4916 | Loss: 1.2340
Epoch 1 | Batch 200/4916 | Loss: 0.8310
Epoch 1 | Batch 300/4916 | Loss: 0.6849
Epoch 1 | Batch 400/4916 | Loss: 0.6941
Epoch 1 | Batch 500/4916 | Loss: 0.7242
Epoch 1 | Batch 600/4916 | Loss: 0.5574
Epoch 1 | Batch 700/4916 | Loss: 0.4798
Epoch 1 | Batch 800/4916 | Loss: 0.6180
Epoch 1 | Batch 900/4916 | Loss: 0.4823
Epoch 1 | Batch 1000/4916 | Loss: 0.6397
Epoch 1 | Batch 1100/4916 | Loss: 0.6793
Epoch 1 | Batch 1200/4916 | Loss: 0.6740
Epoch 1 | Batch 1300/4916 | Loss: 0.5156
Epoch 1 | Batch 1400/4916 | Loss: 0.4644
Epoch 1 | Batch 1500/4916 | Loss: 0.6336
Epoch 1 | Batch 1600/4916 | Loss: 0.5809
Epoch 1 | Batch 1700/4916 | Loss: 0.5522
Epoch 1 | Batch 1800/4916 | Loss: 0.4511
Epoch 1 | Batch 1900/4916 | Loss: 0.5586
Epoch 1 | Batch 2000/4916 | Loss: 0.5663
Epoch 1 | Batch 2100/4916 | Loss: 0.3551
Epoch 1 | Batch 2200/4916 | Loss: 0.4766
Epoch 1 | Batch 2300/4916 | Loss: 0.5746
Epoch 1 | Batch 2400/4916 | Loss: 0.6218
Epoch 1 | Batch 2500/4916 | Loss: 0.5258
Epoch 1 | Batch 2600/4916 | Loss: 0.6024
Epoch 1 | Batch 2700/4916 | Loss: 0.5888
Epoch 1 | Batch 2800/4916 | Loss: 0.3581
Epoch 1 | Batch 2900/4916 | Loss: 0.6199
Epoch 1 | Batch 3000/4916 | Loss: 0.5799
Epoch 1 | Batch 3100/4916 | Loss: 0.6085
Epoch 1 | Batch 3200/4916 | Loss: 0.5417
Epoch 1 | Batch 3300/4916 | Loss: 0.5447
Epoch 1 | Batch 3400/4916 | Loss: 0.4671
Epoch 1 | Batch 3500/4916 | Loss: 0.5085
Epoch 1 | Batch 3600/4916 | Loss: 0.5437
Epoch 1 | Batch 3700/4916 | Loss: 0.4604
Epoch 1 | Batch 3800/4916 | Loss: 0.3993
Epoch 1 | Batch 3900/4916 | Loss: 0.6813
Epoch 1 | Batch 4000/4916 | Loss: 0.4338
Epoch 1 | Batch 4100/4916 | Loss: 0.5112
Epoch 1 | Batch 4200/4916 | Loss: 0.4655
Epoch 1 | Batch 4300/4916 | Loss: 0.5042
Epoch 1 | Batch 4400/4916 | Loss: 0.4657
Epoch 1 | Batch 4500/4916 | Loss: 0.5251
Epoch 1 | Batch 4600/4916 | Loss: 0.4290
Epoch 1 | Batch 4700/4916 | Loss: 0.5348
Epoch 1 | Batch 4800/4916 | Loss: 0.4818
Epoch 1 | Batch 4900/4916 | Loss: 0.5110
Epoch [1/5] | Train Loss: 0.5699 | Val Loss: 0.5506
DEBUG: Asking DataLoader for a batch...
Epoch 2 | Batch 0/4916 | Loss: 0.5502
Epoch 2 | Batch 100/4916 | Loss: 0.4595
Epoch 2 | Batch 200/4916 | Loss: 0.4603
Epoch 2 | Batch 300/4916 | Loss: 0.6879
Epoch 2 | Batch 400/4916 | Loss: 0.4977
Epoch 2 | Batch 500/4916 | Loss: 0.4689
Epoch 2 | Batch 600/4916 | Loss: 0.4808
Epoch 2 | Batch 700/4916 | Loss: 0.4597
Epoch 2 | Batch 800/4916 | Loss: 0.4300
Epoch 2 | Batch 900/4916 | Loss: 0.4376
Epoch 2 | Batch 1000/4916 | Loss: 0.4830
Epoch 2 | Batch 1100/4916 | Loss: 0.5498
Epoch 2 | Batch 1200/4916 | Loss: 0.5007
Epoch 2 | Batch 1300/4916 | Loss: 0.5384
Epoch 2 | Batch 1400/4916 | Loss: 0.4101
Epoch 2 | Batch 1500/4916 | Loss: 0.5454
Epoch 2 | Batch 1600/4916 | Loss: 0.3932
Epoch 2 | Batch 1700/4916 | Loss: 0.4961
Epoch 2 | Batch 1800/4916 | Loss: 0.5139
Epoch 2 | Batch 1900/4916 | Loss: 0.6449
Epoch 2 | Batch 2000/4916 | Loss: 0.3633
Epoch 2 | Batch 2100/4916 | Loss: 0.6042
Epoch 2 | Batch 2200/4916 | Loss: 0.6560
Epoch 2 | Batch 2300/4916 | Loss: 0.5223
Epoch 2 | Batch 2400/4916 | Loss: 0.5335
Epoch 2 | Batch 2500/4916 | Loss: 0.5229
Epoch 2 | Batch 2600/4916 | Loss: 0.5195
Epoch 2 | Batch 2700/4916 | Loss: 0.5568
Epoch 2 | Batch 2800/4916 | Loss: 0.5032
Epoch 2 | Batch 2900/4916 | Loss: 0.4818
Epoch 2 | Batch 3000/4916 | Loss: 0.4911
Epoch 2 | Batch 3100/4916 | Loss: 0.4777
Epoch 2 | Batch 3200/4916 | Loss: 0.4215
Epoch 2 | Batch 3300/4916 | Loss: 0.5686
Epoch 2 | Batch 3400/4916 | Loss: 0.5129
Epoch 2 | Batch 3500/4916 | Loss: 0.3498
Epoch 2 | Batch 3600/4916 | Loss: 0.4631
Epoch 2 | Batch 3700/4916 | Loss: 0.5176
Epoch 2 | Batch 3800/4916 | Loss: 0.5718
Epoch 2 | Batch 3900/4916 | Loss: 0.6221
Epoch 2 | Batch 4000/4916 | Loss: 0.4362
Epoch 2 | Batch 4100/4916 | Loss: 0.3782
Epoch 2 | Batch 4200/4916 | Loss: 0.5224
Epoch 2 | Batch 4300/4916 | Loss: 0.5347
Epoch 2 | Batch 4400/4916 | Loss: 0.5015
Epoch 2 | Batch 4500/4916 | Loss: 0.3083
Epoch 2 | Batch 4600/4916 | Loss: 0.4317
Epoch 2 | Batch 4700/4916 | Loss: 0.4916
